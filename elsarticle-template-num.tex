%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%

% TESTE
% \documentclass[preprint,12pt]{elsarticle}


%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
\documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols

\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{float}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{xcolor}

\usepackage{multicol}
\usepackage{tabularx}


\usepackage{longtable} 
\usepackage{booktabs} 
\usepackage{array}
\usepackage{placeins}
% \usepackage{multirow}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{multirow} 


%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Oral Surgery, Oral Medicine, Oral Pathology and Oral Radiology}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{A Deep Learning Pipeline for Tooth Identification
and Automated Clinical Reporting on Panoramic
X-ray Images}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Rodrigo Rocha Gomes, Héctor Azpúrua} %% Author name

%% Author affiliation
\affiliation{organization={Placeholder},%Department and Organization
            addressline={Placeholder}, 
            city={Belo Horizonte},
            postcode={Placeholder}, 
            state={Minas Gerais},
            country={Brazil}}

%% Abstract
\begin{abstract}
%% Text of abstract
Abstract text.
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%
\input{sections/1_intro}
\input{sections/2_related_works} 


\section{Materials and Methods}

\subsection{Data collection and annotation}

\begin{figure*}[t]
    \centering
    % Ajuste do espaço superior
    % \vspace{-4mm}
    
    % \textwidth garante que a imagem ocupe a largura total das duas colunas
    \includegraphics[width=1\textwidth]{img/dataset/class_distribution.png}
    
    % Ajuste do espaço entre imagem e legenda
    % \vspace{-2mm}
    \caption{Class distribution of the panoramic bounding box dataset, separated into three groups: (i) wisdom teeth, (ii) maxilla, and (iii) mandible.} 
    \label{fig:class-dist}
    
    % Ajuste do espaço inferior
    % \vspace{-5mm}
\end{figure*}

The dataset employed in this study consists of 14627 panoramic radiographs, furnished by a company specializing in software development for dental radiology. The data are fully anonymized, contain no personal information, and were submitted for review to the Research Ethics Committee of the Federal University of Minas Gerais. The image acquisition process involved distinct panoramic radiography devices, originating from multiple manufacturers, rather than a single equipment model. 

The full dataset comprises 32 classes, representing all permanent teeth of the dental arch. Among these, four classes correspond to third molars or wisdom teeth Figure~\ref{fig:class-dist}, which are known to exhibit lower incidence rates compared to other teeth \cite{nanda1954agenesis}. The annotations were performed by dentists who identified each tooth using the FDI \cite{iso:3950:2016} numbering system and determined its long axis (a central point between the crown and the root). 

A visualization of raw samples from the dataset is depicted in Figure~\ref{fig:sampledata}, where features  such as missing tooth, fillings, cavities and implants can be observed.

\begin{figure}[htbp]
    \centering
    % Ajuste para aproximar a figura do texto anterior
    %\vspace{-5mm}
    
    % Linha 1
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/examespapaizassociados.com.br_21590060_1569070.jpg}
    }
    \hfill
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/examespapaizassociados.com.br_21590506_1569101.jpg}
    }
    
    %\vspace{-2mm} % Ajuste o espaço vertical entre as linhas de imagens
    
    % Linha 2
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/examespapaizassociados.com.br_21590721_1569120.jpg}
    }
    \hfill
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/examespapaizassociados.com.br_21590893_1569135.jpg}
    }
    
    %\vspace{-2mm} % Ajuste o espaço vertical entre as linhas de imagens
    
    % Linha 3
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/radioclindigital.radiomemory.com.br_187222_14164.jpg}
    }
    \hfill
    \subfloat{%
        \includegraphics[trim={22cm 5cm 22cm 15cm},clip,
        width=0.48\columnwidth, height=2.3cm]{img/panoramics/radiography/examespapaizassociados.com.br_21623283_1571640.jpg}
    }
    
    % Ajuste para aproximar a legenda das imagens
    %\vspace{-3mm}
    \caption{Raw samples from the panoramic X-ray dataset: many types of panoramic radiograph features can be observed such as missing tooth, fillings, cavities and implants.}
    %\textcolor{red}{capaz adicionar duas imagens mais para preencher o espaço?!}
    \label{fig:sampledata}
    % Ajuste para aproximar o texto seguinte da legenda
    %\vspace{-5mm}
\end{figure}

%\subsection{Data annotation}

Exclusion criteria were applied to the initial dataset to ensure data quality. Images presenting severe positioning errors, motion artifacts, or extensive metal artifacts that compromised the region of interest were removed. Furthermore, a manual verification was subsequently performed to validate the bounding boxes and prevent under and over-classification.

% --- falar do laudo 

\subsection{Clinical report data}

% For this exploratory study, a validation dataset was constituted,

A dataset of clinical text reports with radiographs, was created from 40 randomly selected panoramic radiographs from our test dataset. The data annotation process was conducted by two specialists in Oral and Maxillofacial Radiology. These specialists performed a meticulous analysis of each radiograph to identify and label all pathological findings and anatomical structures of interest. Figure \ref{fig:report_distribution} shows the number of occurrences present in each annotation file.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\columnwidth]{img/plots/ocorrencias_rowchart.png}
    \caption{Distribution of annotations of findings and structures in the 40 panoramic radiographs.}
    \label{fig:report_distribution}
\end{figure}

A cross-review protocol was adopted to minimize labeling errors: each set of examinations was annotated by one specialist and subsequently reviewed by the second. Any disagreements were resolved by consensus; cases where consensus could not be reached were excluded from the sample.  The resulting consensus annotations served as the ground truth for the subsequent validation of the ChatGPT-5 model~\cite{OpenAI2025}. 

% \begin{figure}[H]
%     \centering
%     % (Opcional) Reduz espaço entre texto anterior e imagem
%     \vspace{-3mm} 
    
%     \includegraphics[width=0.72\columnwidth]{img/plots/ocorrencias_rowchart.png}
    
%     % (Opcional) Reduz espaço entre imagem e legenda
%     \vspace{-3mm}
    
%     \caption{Distribution of annotations of findings and structures in the 40 panoramic radiographs.}
%     \label{fig:report_distribution}
    
%     % Reduz significativamente o espaço para o texto seguinte
%     \vspace{-6mm} 
% \end{figure}

\subsubsection{Segmented data}

A subset of the data was manually segmented to allow evaluation of the segmentation component of the proposed pipeline. This dataset comprised 830 grayscale images with dimensions of 2835x1477 pixels. Sample images and masks are depicted in Figure~\ref{fig:labelssegment}. These data were utilized to train and evaluate a YOLOv11 Ultralytics segmentation model \cite{yolo11_ultralytics}. To ensure data consistency, the specific training, validation, and testing subsets defined for the YOLOv11 model were subsequently applied to the fine-tuning of the SAM model.

\begin{figure}[htbp]
    \centering
    % Ajuste para aproximar do texto anterior
    %\vspace{-5mm} 
    
    % Imagem Superior (Original)
    \subfloat[Original x-ray image.]{%
        % Mudei para width=\columnwidth para aproveitar a largura total
        \includegraphics[trim={4cm 2cm 4cm 1cm},clip,width=0.9\columnwidth]{img/panoramics/segmentation/img/ararasradiodonto.radiomemory.com.br_373829_11994.jpg}
    }
    
    % Quebra de linha para forçar a próxima imagem para baixo
    \par 
    %\vspace{-2mm} % Espaço entre as duas imagens
    
    % Imagem Inferior (Máscara)
    \subfloat[Segmentation mask.]{%
        \includegraphics[trim={4cm 2cm 4cm 1cm},clip,width=0.9\columnwidth]{img/panoramics/segmentation/binary_mask/ararasradiodonto.radiomemory.com.br_373829_11994.png}
    }
    
    % Ajuste para aproximar a legenda
    %\vspace{-3mm} 
    % Atualizei a legenda para refletir a nova disposição (Top/Bottom)
    \caption{Sample from the segmented panoramic X-ray dataset: (a) Original x-ray image, and (b) Segmentation mask.}
    \label{fig:labelssegment}
    
    % Ajuste para aproximar do texto seguinte
    %\vspace{-5mm} 
\end{figure}

\subsection{Transforming simple labels into bounding boxes}

One of the main problems in annotating dental data is the complexity of the labels, specially segmentation masks. This is also reflected in the lack of large, publicly accessible, and consistently annotated dental datasets. In this vein, the long axis were labeled in our dataset, as shown in Figure~\ref{fig:labelstransform}.a, a task that is simpler to perform.

To estimate bounding boxes compatible with the object detection models such as YOLO, the average length and height of each of the tooth classes were calculated from the dataset, and then, the centroid of the two points of the manual label was used as the centroid of a box containing this average length and height, plus a slight offset. This method allowed us to drastically increase the size of our dataset by not requiring extra manual labelling. All labels were checked before training to remove outliers in the data where the boxes did not align correctly to the class.

\begin{figure}[htbp]
    \centering
    % Reduz o espaço acima da figura para colá-la no texto anterior
    %\vspace{-5mm} 
    
    % Primeira Imagem
    \subfloat[Annotation of only two points along the axis between the crown.]{%
        % width=\columnwidth garante que use a largura total da coluna
        % Se ficar muito grande, reduza para 0.8\columnwidth
        \includegraphics[width=0.9\columnwidth]{img/label/longaxis_plot_v0.png}
    }
    
    % Força a quebra de linha para a próxima imagem ficar embaixo
    \par 
    %\vspace{-2mm} % Espaço entre os dois plots
    
    % Segunda Imagem
    \subfloat[Final estimated bounding box.]{%
        \includegraphics[width=0.9\columnwidth]{img/label/longaxis_plot_v1.png}
    }
    
    % Aproxima a legenda das imagens
    %\vspace{-3mm} 
    \caption{The data annotation and bounding box estimation process.}
    \label{fig:labelstransform}
    
    % Aproxima o texto seguinte da legenda
    %\vspace{-5mm} 
\end{figure}

\subsection{Pre-processing}

%To ensure compatibility with the YOLO framework, the original JSON labels were parsed and reformatted. Each annotation was transformed into the required structure, consisting of a class identifier followed by the normalized coordinates of its bounding box.

% The exclusion criteria for image selection were as follows: (i) radiographs with missing or incomplete annotations, and (ii) images depicting mixed dentition, characterized by the concurrent presence of both deciduous (primary) and permanent teeth.

% %\subsubsection{Data splitting}

% This study utilized two distinct datasets, one for object detection and one for segmentation. For the training and evaluation of the YOLOv11 and YOLOv12 object detection models, the primary dataset, comprising 14,537 images, was randomly partitioned. The split allocated 80\% (11.631 images) to the training set, 10\% (1.453 images) to the validation set, and 10\% (1.453 images) to the test set.

% For the training and evaluation of the YOLOv11 segmentation network, a specific dataset for this task, containing 830 images, was employed. This set was divided using an 80/10/10 ratio, resulting in 664 images for training, 83 for validation, and 83 for testing. These same training and validation sets were also utilized to perform the fine-tuning of the SAM1 model. Subsequently, the same test set (83 images) was used to evaluate and compare the segmentation quality of both the fine-tuned SAM1 and the YOLOv11.


The image selection process applied specific exclusion criteria: (i) radiographs exhibiting missing or incomplete annotations, and (ii) instances of mixed dentition, characterized by the concurrent presence of deciduous and permanent teeth.


To facilitate the experimental framework, two distinct datasets were curated. The object detection dataset, comprising 14,537 images, was employed to train and evaluate YOLOv11 and YOLOv12. This dataset was randomly partitioned using an 80:10:10 split, allocating 11,631 images for training, 1,453 for validation, and 1,453 for testing.

For the segmentation task, a specialized dataset of 830 images was utilized. Following the same distribution ratio, this set yielded 664 images for training, 83 for validation, and 83 for testing. These partitions were used to train the YOLOv11 segmentation network and to fine-tune the SAM1 model. Consequently, the segmentation performance of both architectures was evaluated and compared using the identical test set of 83 images to ensure a fair benchmark.



\subsubsection{Data augmentation}

Data augmentation plays a crucial role in generating synthetic data and enhancing final model performance~\cite{chlap2021review}. In the context of dental panoramic radiographs, careful selection of augmentation strategies is essential, as the source images are grayscale and acquired at fixed positions via standardized imaging protocols using an orthopantomograph.

% (\textit{hsv\_v}=$0.4$)
% (mosaic=0.5)
% (mixup=True)

The brightness variation was configured as 40\%, meaning that the intensity of the brightness is randomly selected within -0.4 to 0.4, introducing mild alterations in illumination that improve model robustness to exposure differences across radiographic acquisitions. A Mosaic augmentation was applied with 50\% probability, combining four radiographic images into a single composite. This approach enhances detection of small anatomical structures, such as roots or developing teeth, by presenting them in diverse spatial contexts. A random erasing with strategy with 20\% was applied, which helps models learn more robust features and prevents over-reliance on specific image regions. Finally, Mixup was enabled, a technique that improves generalization by creating a composite sample from the linear interpolation of two images and their labels, thus introducing controlled label noise and greater visual variability.



\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\columnwidth]{img/plots/augmentations.jpg}
    \caption{Sample image before (blue) and after (green) the application of data augmentation strategies.}
    \label{fig:report_distribution}
\end{figure}





\subsection{Clinical report pipeline}

The system employs a two-stage pipeline for the analysis of panoramic radiography images. Initially, the YOLOv12x network performs inference on the input radiographic image. The selection of this architecture relies on the metrics presented in Table \ref{tab:yolov11_yolov12_SAM}, specifically the Intersection over Union (IoU) of 0.8514, which represents the highest value among the evaluated models. This inference yields detection outputs comprising both class labels and corresponding bounding boxes Figure~\ref{fig:pipeline_diagram}. These outputs from the YOLOv12x architecture identify the presence or absence of individual teeth within the image. Subsequently, the generated bounding boxes serve as prompts for the SAM1-FT network. This model underwent a fine-tuning process to optimize domain adaptation and segmentation accuracy specific to the dataset. Experimental results indicated that the SAM1-FT architecture achieved the highest performance IoU metrics compared to SAM variants that did not undergo fine-tuning. The network processes these bounding box prompts to produce semantic segmentation of the dental structures, completing the sequential detection and segmentation pipeline.



% \textcolor{red}{Falta explicar qual yolo é usada, e porque. Em todo o texto de materiais e métodos não é explicado de forma detalhada quais modelos são usados em cada etapa}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{img/pipeline_yolosam_journal.png}
    \caption{Proposed multi-step pipeline for teeth classification, segmentation and report generation.}
    \label{fig:pipeline_diagram}
\end{figure*}

Following the inference stage using the YOLO model, the system generates an output in JSON (JavaScript Object Notation) format present in Figure \ref{fig:jsonex1}. This structured file provides a classification specifying the status of each tooth as either "present" or "absent", serving as an assistive tool for the dental professional during examination or charting.

Upon generation of the segmented radiography and the JSON file detailing tooth status, both artifacts are transmitted to a Large Language Model (LLM). The segmented image functions as a visual prompt, while the JSON data serves as a textual prompt. This multimodal interaction employs the prompt engineering techniques illustrated in Table~\ref{tab:prompt_strategies} and detailed in Appendix~\ref{tab:appendix_all_prompts}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\columnwidth]{img/json_ex1.jpg}
    \caption{Teeth detection output: JSON format image description with the classification of present and absent teeth.}
    \label{fig:jsonex1}
\end{figure}




\section{Results}

% -- Falando do treinamento

The training environment consisted of an NVIDIA RTX 6000 (48GB) GPU, an Intel Xeon Silver 4416 (3.9GHz, 80 threads) CPU, and 512GB of RAM, running Ubuntu 24.04 in a Docker Rootless container. The YOLOv11 and YOLOv12 models were fine-tuned on COCO dataset~\cite{lin2014microsoft} utilizing the Ultralytics library (version 8.3.0). Key hyperparameters included an input image size of 1024x1024 pixels, a batch size of 8, and an initial learning rate of 0.00125.

Models were trained for a maximum of 150 epochs, utilizing an early stopping patience of 30 epochs to prevent overfitting. The Distributed Focal Loss (DFL) of the YOLOv11 and YOLOv12 training can be observed in Figure~\ref{fig:val_dfl_loss}.


% \textcolor{red}{Trocar legenda \ref{fig:prompt_guided}}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{img/val_dfl_loss_comparison.png}
%     \caption{Validation Distribution Focal Loss (DFL) per epoch for the YOLOv11
% model training.}
%     \label{fig:val_dfl_loss}
% \end{figure*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\columnwidth]{img/val_dfl_loss_comparison.png}
    \caption{Validation Distribution Focal Loss (DFL) per epoch for the YOLOv11 and YOLOv12 model training.}
    \label{fig:val_dfl_loss}
\end{figure}

%  -- Falando das redes de classificação
\subsection{Quantitative evaluation of architectures and efficiency}

\begin{table*}[th!]
\centering
\small % Reduz levemente a fonte para melhor ajuste na largura total
\caption{Detailed classification performance metrics for each individual tooth using the YOLOv12x architecture.}
% The spacing separates the dental quadrants (Upper Right, Upper Left, Lower Left, Lower Right).
\label{tab:individual_classification}
\setlength{\tabcolsep}{6pt} % Aumenta levemente o espaçamento horizontal para preencher a largura
\begin{tabular}{lrrrrrrrr}
\toprule
\textbf{Teeth} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{mAP\textsubscript{50}} & \textbf{mAP\textsubscript{50-95}} \\ 
\midrule
\multicolumn{9}{l}{\small \textit{Upper right quadrant}} \\
11 & 1327 & 7  & 5  & 0.9948 & 0.9962 & 0.9955 & 0.9962 & 0.8012 \\
12 & 1308 & 5  & 8  & 0.9962 & 0.9939 & 0.9951 & 0.9937 & 0.7930 \\
13 & 1353 & 17 & 6  & 0.9876 & 0.9956 & 0.9916 & 0.9928 & 0.7983 \\
14 & 1182 & 29 & 21 & 0.9761 & 0.9825 & 0.9793 & 0.9789 & 0.7177 \\
15 & 1156 & 29 & 17 & 0.9755 & 0.9855 & 0.9805 & 0.9820 & 0.7586 \\
16 & 1181 & 18 & 5  & 0.9850 & 0.9958 & 0.9904 & 0.9918 & 0.7850 \\
17 & 1220 & 36 & 21 & 0.9713 & 0.9831 & 0.9772 & 0.9790 & 0.7859 \\
18 & 641  & 27 & 14 & 0.9596 & 0.9786 & 0.9690 & 0.9749 & 0.7639 \\ 
\addlinespace
\multicolumn{9}{l}{\small \textit{Upper left quadrant}} \\
21 & 1316 & 5  & 3  & 0.9962 & 0.9977 & 0.9970 & 0.9975 & 0.8045 \\
22 & 1306 & 13 & 6  & 0.9901 & 0.9954 & 0.9928 & 0.9953 & 0.7934 \\
23 & 1342 & 14 & 5  & 0.9897 & 0.9963 & 0.9930 & 0.9950 & 0.7910 \\
24 & 1212 & 39 & 17 & 0.9688 & 0.9862 & 0.9774 & 0.9783 & 0.6908 \\
25 & 1124 & 40 & 17 & 0.9656 & 0.9851 & 0.9753 & 0.9783 & 0.7366 \\
26 & 1170 & 23 & 14 & 0.9807 & 0.9882 & 0.9844 & 0.9808 & 0.7671 \\
27 & 1207 & 43 & 29 & 0.9656 & 0.9765 & 0.9710 & 0.9654 & 0.7639 \\
28 & 637  & 32 & 14 & 0.9522 & 0.9785 & 0.9652 & 0.9626 & 0.7442 \\ 
\addlinespace
\multicolumn{9}{l}{\small \textit{Lower left quadrant}} \\
31 & 1407 & 21 & 6  & 0.9853 & 0.9958 & 0.9905 & 0.9915 & 0.7595 \\
32 & 1417 & 14 & 3  & 0.9902 & 0.9979 & 0.9940 & 0.9935 & 0.7827 \\
33 & 1434 & 10 & 3  & 0.9931 & 0.9979 & 0.9955 & 0.9967 & 0.8149 \\
34 & 1333 & 14 & 7  & 0.9896 & 0.9948 & 0.9922 & 0.9915 & 0.8251 \\
35 & 1252 & 15 & 8  & 0.9882 & 0.9937 & 0.9909 & 0.9922 & 0.8495 \\
36 & 938  & 14 & 13 & 0.9853 & 0.9863 & 0.9858 & 0.9816 & 0.8770 \\
37 & 1096 & 40 & 23 & 0.9648 & 0.9794 & 0.9721 & 0.9750 & 0.8624 \\
38 & 706  & 40 & 12 & 0.9464 & 0.9833 & 0.9645 & 0.9759 & 0.8044 \\ 
\addlinespace
\multicolumn{9}{l}{\small \textit{Lower right quadrant}} \\
41 & 1401 & 19 & 11 & 0.9866 & 0.9922 & 0.9894 & 0.9892 & 0.7589 \\
42 & 1408 & 17 & 9  & 0.9881 & 0.9936 & 0.9909 & 0.9930 & 0.7896 \\
43 & 1433 & 15 & 4  & 0.9896 & 0.9972 & 0.9934 & 0.9957 & 0.8200 \\
44 & 1352 & 23 & 5  & 0.9833 & 0.9963 & 0.9898 & 0.9926 & 0.8277 \\
45 & 1242 & 22 & 15 & 0.9826 & 0.9881 & 0.9853 & 0.9872 & 0.8516 \\
46 & 965  & 29 & 10 & 0.9708 & 0.9897 & 0.9802 & 0.9829 & 0.8721 \\
47 & 1096 & 48 & 34 & 0.9580 & 0.9699 & 0.9639 & 0.9631 & 0.8628 \\
48 & 711  & 24 & 29 & 0.9673 & 0.9608 & 0.9641 & 0.9548 & 0.7981 \\ 
\bottomrule
\end{tabular}
\end{table*}
The experimental results for the YOLOv11 and YOLOv12 model families are presented in Table~\ref{tab:yolov11_yolov12}, respectively. Data analysis indicates that both architectures achieved detection performance with minimal variation between them. Metrics such as Precision, Recall, and $mAP_{50}$ remained at values around $0.98$ for nearly all variants. This parity was maintained in the $mAP_{50-95}$ metric, where all models scored within the $0.790$ to $0.797$ range. A performance increase in this metric was observed as the model size increased (e.g., YOLOv11l and YOLOv12l). The differentiating factor among the models was inference speed (FPS). The YOLOv11n ($44.41$ FPS) and YOLOv11s ($44.08$ FPS) variants demonstrated throughput levels that exceeded the v12 family models (YOLOv12s at $41.04$ FPS). Considering the parity in detection accuracy between the families, the YOLOv11 models, specifically the 'n' and 's' variants, presented the best trade-off relationship between performance and computational efficiency for this task. 



\textcolor{red}{Sera que a YOLOv11 é melhor então? Acho que não... Acho que eu usaria o que tem mais precision na nova arquitetura num FPS aceitavel, que é a yolov12n, e isso complementa os proximos experimentos...}
\textcolor{blue}{A YOLOv12n apresenta as melhores métricas para detecção (por isso escolhi para realizar a inferencia da tabela 3), mas para realizar a segementação, a YOLOv12x possui a melhor IoU.}

To assess the performance at a granular level, Table~\ref{tab:individual_classification} details the classification metrics for each individual tooth obtained using the YOLOv12x model. This architecture was selected for the detailed evaluation as it achieved the highest IoU in SAM1-FT (0.8514) among the evaluated models. Maximizing the IoU is prioritized in this pipeline to ensure segmentation accuracy before the transmission of these outputs as visual prompts to the LLM. The model demonstrated stability across the varying morphological structures of the dental arch. The F1-scores ranged from a minimum of $0.9639$ (tooth 47) to a maximum of $0.9970$ (tooth 21). A distinct pattern was observed where anterior teeth (incisors and canines) yielded higher precision and recall values compared to posterior segments. The reduced performance metrics associated with the third molars (teeth 18, 28, 38, and 48) correlate with the lower incidence of these elements within the dataset, as demonstrated in Figure \ref{fig:class-dist}.


% -------- Falando da comparação da YOLOv8 do baseline vs YOLOv12 proposta
\subsection{Cross-Evaluation and generalization analysis}

\begin{table*}[t]
\centering
\caption{Performance and generalization analysis of the proposed YOLOv12x and the baseline YOLOv8x~\cite{mendes2025automated} across their respective and alternate datasets. Values are presented as mean with standard deviation.}
\label{tab:yolov8_vs_yolov12}

% Ajusta a largura total para a largura do texto
\begin{tabularx}{\textwidth}{@{}Xcccc@{}} 
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP\textsubscript{50}} & \textbf{mAP\textsubscript{50-95}} \\ 
\midrule
Baseline~\cite{mendes2025automated} \newline (YOLOv8x w/ DENTEX) & 0.8982 ($\pm$ 0.0097) & 0.9340 ($\pm$ 0.0079) & 0.8996 ($\pm$ 0.0113) & 0.4892 ($\pm$ 0.0083) \\
Baseline~\cite{mendes2025automated} \newline (YOLOv8x w/ our dataset) & 0.6789 ($\pm$ 0.1931) & 0.7771 ($\pm$ 0.1877) & 0.6340 ($\pm$ 0.2446) & 0.2087 ($\pm$ 0.0910) \\ 
\addlinespace
YOLOv12x (Our dataset) & \textbf{0.9808} ($\pm$ 0.0609) & \textbf{0.9897} ($\pm$ 0.0486) & \textbf{0.9843} ($\pm$ 0.0112) & \textbf{0.7953} ($\pm$ 0.0436) \\ 
YOLOv12x (DENTEX dataset) & 0.8702 ($\pm$ 0.1691) & 0.8838 ($\pm$ 0.1672) & 0.8208 ($\pm$ 0.0641) & 0.2972 ($\pm$ 0.0371) \\ 
\bottomrule
\end{tabularx}
\end{table*}

To compare the performance and generalization capability of the bounding boxes from the proposed YOLOv12x architecture, we use the work of Mendes \textit{et al.}~\cite{mendes2025automated}, which uses YOLOv8x, as a baseline for a cross-evaluation. \textcolor{orange}{According to Table~\ref{tab:yolov8_vs_yolov12}, the YOLOv12x model exhibits reduced performance when evaluated on the DENTEX dataset (used by the baseline). An analogous phenomenon occurs with the YOLOv8x model on this study's dataset: the proposed model demonstrates generalization capability for images outside the training set.} \textcolor{red}{Revisar este texto, esta confuso, al final quem é melhor? O nosso bate em todos os casos! tem que deixar isso claro. Acho que o problema com os bounding boxes nem deveria ser comentado, já que o v12 é melhor. }

\textcolor{blue}{IMPORTANTE: Por que acima na Tabela 3 foi usado v12n e aqui v12x? Tem que ser o mesmo modelo. Você pode colocar os dados do v12n? Tem que ser usado o mesmo modelo desde a subseção 4.1 em frente, é pra isso que foram realizados os experimentos das diferentes versões.}

\sout{The difficulty in comparison originates from the discrepancy in the ground truth definitions. Figure \ref{fig:plot_bbox_dentex} illustrates this divergence: Figure~\ref{fig:plot_bbox_dentex}.a displays the bounding boxes after inference using the model by Mendes \textit{et al.}, while Figure~\ref{fig:plot_bbox_dentex}.b presents the results from the proposed YOLOv12x. The visual comparison highlights the distinct bounding box dimensions characteristic of each model. The model in this study was trained using bounding boxes with a smaller width. This results in an underestimation of performance metrics when compared to the baseline, which was trained with wider bounding boxes. The reciprocal effect is observed when evaluating the YOLOv8x model on this study's dataset.}

\begin{figure}[htbp]
    \centering
    % Reduz o espaço acima para colar no texto anterior
    %\vspace{-5mm}
    
    % Primeira Imagem (Topo)
    \subfloat[Prediction by Mendes \textit{et al.}~\cite{mendes2025automated}]{%
        % width=\columnwidth garante o uso total da largura da coluna
        \includegraphics[width=\columnwidth]{img/comparison_dentex/dentex_train_189.jpg}
    }
    
    % Força a quebra de linha para empilhar
    \par 
    %\vspace{-2mm} % Espaço entre as duas imagens
    
    % Segunda Imagem (Baixo)
    \subfloat[Prediction by the proposed method.]{%
        \includegraphics[width=\columnwidth]{img/comparison_dentex/yolo12_train_189.jpg}
    }
    
    % Reduz o espaço entre imagens e legenda
    %\vspace{-3mm}
    \caption{Comparison of bounding box dimensions sizes.}
    \label{fig:plot_bbox_dentex}
    
    % Reduz o espaço abaixo para colar no texto posterior
    %\vspace{-5mm}
\end{figure}


%  ------ Falando das redes de segmentação ------
\subsection{Accuracy and efficiency trade-off in foundation models}
Now with the goal of evaluating the capacity of SAM networks, the Table \ref{tab:yolov11_yolov12_SAM} present the segmentation performance results for combinations of YOLOv11 and YOLOv12 detector backbones with four segmentation models: SAM1, SAM2, SAM1-FT and HQ-SAM\cite{ke2023segmenthighquality}. The analysis of the IoU metric indicates that SAM1-FT yielded the highest values across all configurations, with scores consistently above $0.8375$. Conversely, the choice between the YOLOv11 and YOLOv12 backbones did not introduce significant differences in either IoU or inference time for any given segmentation model. Regarding computational cost, SAM2 exhibited the highest inference times, exceeding $1400$ ms in all tests. HQ-SAM recorded the lowest mean inference times, particularly with smaller YOLO variants (e.g., YOLOv11n: $338$ ms), though it also registered high standard deviations. The SAM1 and SAM1-FT models showed comparable processing times, remaining in the $580-630$ ms range. SAM1-FT, therefore, provided the highest IoU while maintaining an inference time similar to the baseline SAM1, representing a specific trade-off between segmentation accuracy and computational cost.


\begin{table*}[t] 
\centering
\caption{SAM variations comparison: IoU scores and inference times in milliseconds (ms).}
\label{tab:yolov11_yolov12_SAM}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l cc cc cc cc @{}}
\toprule
\multirow{\textbf{Model}} & \multicolumn{2}{c}{\textbf{SAM1}} & \multicolumn{2}{c}{\textbf{SAM2}} & \multicolumn{2}{c}{\textbf{SAM1-FT}} & \multicolumn{2}{c}{\textbf{HQ-SAM}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& IoU & Time (ms) & IoU & Time (ms) & IoU & Time (ms) & IoU & Time (ms) \\
\midrule


YOLOv11n       & 0.7853 $\pm$ 0.0897 & \textbf{583 $\pm$ 123} & 0.8047 $\pm$ 0.0838 & \textbf{1423.213 $\pm$ 599.04}  & {0.8375 $\pm$ 0.0713}  & 590 $\pm$ 116 & 0.7984 $\pm$ 0.0877 & \textbf{338   $\pm$ 95}  \\
YOLOv11s       & 0.7959 $\pm$ 0.0768 & 586 $\pm$ 129 & \textbf{0.8086 $\pm$ 0.0658} & 1547.311 $\pm$ 633.978 & 0.8428 $\pm$ 0.0608  & \textbf{581 $\pm$ 99}  & 0.8086 $\pm$ 0.0732 & 379  $\pm$ 217  \\
YOLOv11m       & 0.8091 $\pm$ 0.0788 & 593 $\pm$ 144 & 0.8027 $\pm$ 0.0663 & 1627.114 $\pm$ 623.661 & 0.8464 $\pm$ 0.0566  & 598 $\pm$ 125 & 0.8200 $\pm$ 0.0737 & \textbf{338 $\pm$ 95}    \\
YOLOv11l       & \textbf{0.8162} $\pm$ 0.0762 & 621 $\pm$ 155 & 0.8064 $\pm$ 0.0614 & 1804.451 $\pm$ 798.397 & \textbf{0.8488 $\pm$ 0.0561}  & 629 $\pm$ 158 & \textbf{0.8254 $\pm$ 0.0713} & 662 $\pm$ 754   \\
YOLOv11x       & 0.7910 $\pm$ 0.0863 & 611 $\pm$ 191 & 0.8065 $\pm$ 0.0705 & 1596.999 $\pm$ 615.439 & 0.8393 $\pm$ 0.0658  & 611 $\pm$ 174 & 0.8028 $\pm$ 0.0841 & 608 $\pm$ 939   \\ \hline
YOLOv12n       & 0.8072 $\pm$ 0.0763 & \textbf{586 $\pm$ 137} & 0.7978 $\pm$ 0.0734 & 1625.155 $\pm$637.352  & 0.8437 $\pm$ 0.0588  & \textbf{577 $\pm$ 98}  & 0.8180 $\pm$ 0.0712 & \textbf{348 $\pm$ 106}   \\
YOLOv12s       & 0.7949 $\pm$ 0.0796 & 589 $\pm$ 143 & 0.8001 $\pm$ 0.0755 & 1747.213 $\pm$ 685.401 & 0.8412 $\pm$ 0.0613  & 585 $\pm$ 104 & 0.8075 $\pm$ 0.0765 & 377 $\pm$ 181   \\
YOLOv12m       & 0.8054 $\pm$ 0.0794 & 595 $\pm$ 144 &\textbf{ 0.8027 $\pm$ 0.0679} & 1615.633 $\pm$ 553.313 & 0.8426 $\pm$ 0.0584  & 593 $\pm$ 110 & 0.8159 $\pm$ 0.0755 & 353 $\pm$ 113   \\
YOLOv12l       & 0.8270 $\pm$ 0.0690 & 605 $\pm$ 172 & 0.7898 $\pm$ 0.0672 & 1691.498 $\pm$ 645.422 & 0.8489 $\pm$ 0.0542  & 603 $\pm$ 133 & 0.8331 $\pm$ 0.0672 & 688 $\pm$ 992   \\
YOLOv12x       & \textbf{0.8343 $\pm$ 0.0681} & 611 $\pm$ 206 & 0.7914 $\pm$ 0.0644 &\textbf{1579.951 $\pm$ 589.140}  & \textbf{0.8514 $\pm$ 0.0544}  & 594 $\pm$ 155 & \textbf{0.8388 $\pm$ 0.0671} & 689 $\pm$ 1.227 \\ 
\bottomrule
\end{tabular}
}
\end{table*}


% -------- Falando da comparação da U-net do baseline vs YOLOv12 proposta
\subsection{Comparative analysis with baseline segmentation methods}
% Neste teste foi utilizado o dataset do baseline a fim de comparação
We also conducted a comparative test between the U-Net network from the work of Zhang \textit{et al.}~\cite{zhang2023children} and our proposed SAM1-FT and YOLOv11s-SEG networks trained for segmentation. The YOLOv11s-SEG variant was selected for this evaluation as it presented the most favorable trade-off within the architecture family. According to Table~\ref{tab:unet_sam1_yolov11s_comparison}, it is observable that both proposed models achieved higher IoU values than the baseline. The YOLOv11s-SEG model exhibited the highest IoU ($0.8707 \pm 0.0263$), followed by SAM1-FT ($0.8514 \pm 0.0544$) and the U-Net ($0.7803 \pm 0.0329$). Regarding computational efficiency, the U-Net recorded the lowest inference time ($12.20 \pm 1.48$ ms). The YOLOv11s-SEG model required $44.66$ ($\pm 57.81$) ms, while the SAM1-FT model demonstrated the highest computational cost, with an average time of $594$ ($\pm 155$) ms. A central methodological advantage of the SAM1-FT model is its capability to generate segmentations without requiring a training dataset with pre-annotated segmentation masks. This fundamentally differentiates it from the U-Net and YOLOv11s-SEG approaches, which necessitate such annotations (pixel-level labels) for supervised training.

\begin{table}[htbp]
    \centering
    \caption{Comparative performance analysis of the proposed SAM1-FT, YOLOv11 segmentation and the baseline U-Net (Zhang et al. 2023).}
    \label{tab:unet_sam1_yolov11s_comparison}
    % \resizebox{\columnwidth}{!}{%
    % }
    \small
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Model} & \textbf{IoU} & \textbf{Time (ms)} \\
        \midrule
        U-Net & 0.7803 $\pm$ 0.0329 & 12.20 $\pm$ 1.48 \\
        SAM1-FT & 0.8514 $\pm$ 0.0544 & 594 $\pm$ 155 \\
        YOLOv11s-SEG & 0.8707 $\pm$ 0.0263 & 44.66 $\pm$ 57.81 \\
        \bottomrule
    \end{tabular}
\end{table}

% ---- Falando da segmentação ultralytics da YOLOv11
\subsection{Supervised segmentation performance and annotation requirements}

Additionally, an experiment was conducted utilizing the native instance segmentation capabilities of the YOLOv11-SEG architecture. The results for this approach, detailed in Table \ref{tab:yolov11_segmentation}, indicate that the YOLOv11 segmentation model achieved superior performance compared to the previously analyzed Segment Anything-based networks. The mean IoU, exemplified by the YOLOv11s variant ($0.8707$), surpassed the highest score from the SAM-based combinations (SAM1-FT, $\approx 0.849$). This performance, however, is contingent upon a specific data prerequisite: the model requires training on polygonal mask annotations, which are significantly more labor-intensive to produce than the bounding box data used to prompt the SAM-based models.

\begin{table}[H]
\centering
\caption{YOLOv11 Segmentation performance results on a test dataset of 83 images.}
\label{tab:yolov11_segmentation}
\footnotesize
%\resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}lcc@{}}
    \toprule
    \textbf{Model} & \textbf{IoU} & \textbf{Time (ms)} \\
    \midrule
    YOLOv11n & 0.8587 $\pm$ 0.0493 & \textbf{42.64 $\pm$ 51.90} \\
    YOLOv11s & \textbf{0.8707 $\pm$ 0.0263} & 43.66 $\pm$ 57.81 \\
    YOLOv11m & 0.8596 $\pm$ 0.0492 & 46.73 $\pm$ 63.64 \\
    YOLOv11l & 0.8582 $\pm$ 0.0560 & 50.36 $\pm$ 67.94 \\
    YOLOv11x & 0.8690 $\pm$ 0.0447 & 54.09 $\pm$ 96.17 \\
    \bottomrule
    \end{tabular}%
%}
\end{table}



% 

\subsection{Classification using ChatGPT-5}

To assess the capabilities of ChatGPT-5, two experimental protocols were established. Experiment I aimed to classify the dental status regarding the presence or absence of teeth. Experiment II was designed as an extension of the first, evaluating the identification of specific dental conditions and radiographic features within the panoramic images. The methodology incorporated two distinct textual prompting strategies: a zero-shot approach and a guided-prompt approach, as detailed in Table \ref{tab:prompt_strategies}. Furthermore, the influence of visual input was evaluated by contrasting non-segmented panoramic radiographs with segmented images, as illustrated in Figure \ref{fig:panoramic_no_with_seg}. A complete example of the prompt structure is provided in the Appendix (Table \ref{tab:appendix_all_prompts}).


\begin{table}[h!]
\centering
\caption{Prompt Strategies.}
\label{tab:prompt_strategies}
% \columnwidth garante que a tabela ocupe a largura exata da coluna
% 'X' é a coluna que quebra o texto automaticamente
\begin{tabularx}{\columnwidth}{@{}lX@{}} 
\toprule
\textbf{Prompt Strategies} & \textbf{Definition} \\ 
\midrule
Zero-shot prompt & Identify the present and missing teeth, without assistance. \\ 
\addlinespace % Espaçamento essencial quando há quebra de linha em colunas estreitas
Segmentation     & Identify the teeth present and missing, stating that the teeth are segmented on the panoramic radiograph. \\ 
\addlinespace
Guided prompt    & Identify present and missing teeth, defining each anomaly and characteristic that should be identified. \\ 
\bottomrule
\end{tabularx}
\end{table}

The goal of these two experiments was to understand how simpler and more complex prompts influence ChatGPT5's image analysis.


% ----- First experiment -----
% Teeth present and absent

\textcolor{blue}{
In the first experiment, with the aim of identifying only the present and absent teeth, we designed using a single set of 40 panoramic radiographs. Each radiograph was processed under two different conditions: first in its original, unaltered state, and second, augmented with segmentation masks generated by the Segment Anything Fine-Tuned (SAM1-FT) model. The figure \ref{fig:panoramic_no_with_seg} compares the different visual prompts and sample images used for segmentation with ChatGPT-5.
}
\textcolor{blue}{Revisar isso de falar sobre experimentos na seção de metodologia, tudo isso deveria estar na seção de experimentos}

\begin{figure}[H]
    \centering
    % Reduz o espaço vertical acima para colar no texto anterior
    \vspace{-5mm} 
    
    % Primeira Imagem (Topo)
    \subfloat[Panoramic radiography no segmentation.]{%
        \includegraphics[width=\columnwidth]{img/segmentation/panoramic_no_seg.jpg}
    }
    
    % Quebra de linha para empilhar verticalmente
    \par 
    \vspace{-2mm} % Espaço entre as duas imagens
    
    % Segunda Imagem (Baixo)
    \subfloat[Panoramic radiography segmentation.]{%
        \includegraphics[width=\columnwidth]{img/segmentation/panoramic_segm.jpg}
    }
    
    % Aproxima a legenda das imagens
    \vspace{-3mm} 
    \caption{Example of a panoramic radiograph used as a visual prompt for ChatGPT-5. \textcolor{red}{isso deveria estar na seção de experimentos}}
    \label{fig:panoramic_no_with_seg}
    
    % Aproxima o texto seguinte da legenda
    \vspace{-5mm} 
\end{figure}

% ----- Second experiment -----
% Recognizing characteristics

For the second experiment, the LLM was guided using prompt engineering techniques to perform two tasks: quantifying the number of teeth present and recognizing distinct dental characteristics depicted in the radiographic images, according to the table \ref{fig:report_distribution}, classifying each one according to the FDI. This methodology was informed by the strategies demonstrated to enhance multimodal model performance in the work of Xiong et al. \cite{xiong2025exploring}, aiming to unlock a more robust and accurate analytical capability from the model. 









The experimental evaluation was performed using the OpenAI online platform \cite{OpenAI2025}, providing access to the ChatGPT-5 model.






\begin{table*}[t]
\centering
\caption{Comparison of results between YOLOv11 and YOLOv12, accompanied with the standard deviation.}
\label{tab:yolov11_yolov12}
\begin{tabular}{lccccc}

\hline
\textbf{Model} & \textbf{Precision}      & \textbf{Recall}     & \textbf{mAP50}        & \textbf{mAP50-95}      & \textbf{FPS} \\ \hline
YOLOv11n       & 0.9782 (± 0.0603)              & 0.9913 (± 0.0490)          & 0.9880 (± 0.0084)            & 0.7906 (± 0.0432)             & \textbf{44.41}        \\
YOLOv11s       & 0.9794 (± 0.0606)              & 0.9907 (± 0.0495)          & 0.9869 (± 0.0097)            & 0.7944 (± 0.0422)             & 44.08        \\
YOLOv11m       & 0.9796 (± 0.0594)              & 0.9903 (± 0.0482)          & 0.9857 (± 0.0098)            & 0.7963 (± 0.0431)             & 39.90        \\
YOLOv11l       & 0.9783 (± 0.0628)              & 0.9907 (± 0.0472)          & 0.9859 (± 0.0114)            & 0.7974 (± 0.0418)             & 27.96        \\
YOLOv11x       & 0.9790 (± 0.0624)              & 0.9904 (± 0.0489)          & 0.9860 (± 0.0107)            & \textbf{0.7974 (± 0.0432)}    & 36.59        \\ \hline
YOLOv12n       & 0.9783 (± 0.0579)              & \textbf{0.9917 (± 0.0474)} & \textbf{0.9884 (± 0.0094)}   & 0.7911 (± 0.0427)             & 39.74        \\
YOLOv12s       & 0.9776 (± 0.0615)              & 0.9901 (± 0.0501)          & 0.9858 (± 0.0098)            & 0.7943 (± 0.0433)             & 41.04        \\
YOLOv12m       & 0.9778 (± 0.0623)              & 0.9907 (± 0.0481)          & 0.9865 (± 0.0093)            & 0.7970 (± 0.0431)             & 39.19        \\
YOLOv12l       & 0.9800 (± 0.0635)              & 0.9898 (± 0.0497)          & 0.9855 (± 0.0096)            & \textbf{0.7973 (± 0.0430)}    & 30.23        \\
YOLOv12x       & \textbf{0.9808 (± 0.0609)}     & 0.9897 (± 0.0486)          & 0.9843 (± 0.0112)            & 0.7953 (± 0.0436)             & 30.76        \\ \hline
\end{tabular}
\end{table*}




% ----- Primeiro experimento dente presente x ausente
\subsection{Evaluation of visual analytical capabilities of Largue Language Models}

With the aim of evaluating the visual analytical capabilities of ChatGPT-5, the Table \ref{tab:gpt5_absent_present} presents the results for the experiment on identifying present and absent teeth. The data indicate that the 'GPT5 No Segmentation' strategy, which utilized a simpler prompt, obtained Precision ($0.8661$) and Recall ($0.8854$) values. In contrast, the 'GPT5 Segmentation' approach, which provided a visual indication of segmented teeth, registered a reduction in these metrics (Precision $0.8585$, Recall $0.8516$). However, this same approach with visual indication demonstrated a lower inference time ($2928$ ms) compared to the prompt without segmentation ($3276$ ms). Both GPT5 configurations were outperformed by the YOLOv12m detector model, which achieved a Precision of $0.9764$ and a Recall of $0.9548$ with an inference time of $63$ ms.



% Adicione isto no preâmbulo se ainda não tiver:
% \usepackage{graphicx}

\begin{table}[H] % [H] para fixar, ou [h] para fluir na coluna
\centering
\caption{Results of the Comparison between YOLO12m and ChatGPT-5.}
\label{tab:gpt5_absent_present}
\footnotesize
% O resizebox força a tabela a ter a largura exata da coluna de texto
%\resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{Time (ms)} \\
    \midrule
    GPT5 Seg & 0.858 $\pm$ 0.117 & 0.851 $\pm$ 0.234 & 2928 $\pm$ 1227 \\
    GPT5 NoSeg & 0.866 $\pm$ 0.112 & 0.885 $\pm$ 0.187 & 3276 $\pm$ 1187 \\
    YOLOv12m & \textbf{0.976 $\pm$ 0.038} & \textbf{0.954 $\pm$ 0.156} & \textbf{63 $\pm$ 142} \\
    \bottomrule
    \end{tabular}%
%}
\end{table}


% ----- Falando do laudo com ChatGPT5


To enhance the evaluation, we analyze the ability to identify anomalies and structures in the images. Table \ref{tab:appendix_all_prompts} presents the results for the experiment using ChatGPT-5 to identify dental labels. The methodology involved four distinct prompt configurations: Zero-shot and Guided strategies, each applied to non-segmented and segmented panoramic radiographs. For the segmented images, the specific instruction "The teeth are segmented in the image, use them as a support tool" was added to the prompt.

According to Figure \ref{fig:precision_recall_chatgpt5_laudo}, the data indicate that the 'Zero-shot with segmentation' strategy obtained Recall values of 0.7322 for 'Present teeth' and 0.7524 for 'Missing teeth'. In contrast, the 'Guided prompt' approach, which utilized non-segmented images, registered Precision and Recall values of 0.0 for the 'Present teeth' label. However, the 'Zero-shot prompt' configuration demonstrated Precision values of 0.4474 for 'Endodontic treatment' and 0.4286 for 'Presence of implants'. The metrics for 'Crown lesions' did not exceed 0.20 in any configuration.

\begin{figure}[h!]
    \centering
    % Primeira figura (Topo)
    \subfloat[Precision.]{%
        % Apenas width=\linewidth é mantido para redimensionar sem cortar
        \includegraphics[width=\linewidth]{img/plots/comparison_Precision.png}
        \label{fig:precision_results}
    }
    
    \vspace{0.5em} % Espaçamento vertical entre as figuras
    
    % Segunda figura (Abaixo)
    \subfloat[Recall.]{%
        \includegraphics[width=\linewidth]{img/plots/comparison_Recall.png}
        \label{fig:recall_results}
    }

    \caption{Performance Evaluation (Precision and Recall) of GPT-5 on the Expert Report Analysis task.}
    \label{fig:precision_recall_chatgpt5_laudo}
\end{figure}



\section{Discussion}


% Introdução a discussion
The primary objective of this study was to evaluate the performance trade-offs between computational efficiency and diagnostic accuracy in the automated analysis of panoramic radiographs. The investigation covered three distinct domains: object detection using several YOLO architectures, semantic segmentation comparing traditional and foundation models, and the exploratory potential of Large Language Models (LLMs) for visual diagnosis.




% ----- Falando da YOLOv11 e YOLOv12 para classificação e detecção -----
\subsection{Detection architectures and efficiency trade-offs}
The analysis of the detection models (Table \ref{tab:yolov11_yolov12}) indicates that increasing model complexity does not strictly correlate with proportional gains in clinical applicability. While larger architectures such as YOLOv11x and YOLOv12x achieved the highest metrics in complex scenarios (mAP50-95), the gain was marginal compared to the computational cost incurred. The YOLOv11n model emerged as the optimal candidate for real-time applications, delivering a processing speed of 44.41 FPS while maintaining top-tier accuracy (mAP50 of 0.9880). This finding is consistent with recent trends in edge computing for medical imaging, where low latency is prioritized over incremental gains in precision. The comparison suggests that for the specific task of tooth detection, the feature extraction capabilities of the nano and small variants are sufficient, and the architectural overhead of the YOLOv12 family did not translate into a decisive advantage over the YOLOv11 optimizations for this specific dataset.


\subsection{Generalization and annotation standardization}

A critical challenge in medical image analysis is the subjectivity of ground truth definitions. The cross-evaluation performed with the work of Mendes et al. (2025) \cite{mendes2025automated} (Table \ref{tab:yolov8_vs_yolov12}) highlights this issue. The decline in performance observed when models were tested on alternate datasets is attributable to the divergence in annotation styles, specifically the width of the bounding boxes, rather than a failure of the model's generalization capability. This phenomenon emphasizes the necessity for standardized annotation protocols in dental radiology. Despite the discrepancy in intersection-over-union (IoU) based metrics caused by different bounding box margins, the proposed YOLOv12x model demonstrated robustness, maintaining high precision on its native dataset, which suggests that the learned features are stable, albeit sensitive to the specific definitions of the training supervision.


\subsection{Segmentation strategies: The cost of supervision}
The segmentation experiments (Table \ref{tab:unet_sam1_yolov11s_comparison} and Table \ref{tab:yolov11_yolov12_SAM}) offer a perspective on the "cost of annotation" versus performance. The native YOLOv11 segmentation model achieved the highest IoU (0.8707), outperforming both the U-Net baseline (Zhang et al., 2023 \cite{zhang2023children}) and the SAM variants. However, this performance requires a dataset with pixel-level polygonal masks, which are labor-intensive to generate. In contrast, the SAM1-FT model represents a significant methodological advancement. By achieving an IoU of 0.8514 using only bounding boxes as prompts (weak supervision) followed by fine-tuning, it approaches the performance of fully supervised models without the requisite mask annotation cost. This trade-off is crucial for scaling medical datasets. While SAM2 and HQ-SAM introduced architectural novelties, they did not surpass the fine-tuned SAM1 in this specific domain. SAM2, in particular, exhibited inference times (exceeding 1400 ms) that render it impractical for high-throughput clinical workflows, whereas SAM1-FT maintains a balance between the automated prompt capabilities and acceptable inference latency ($\approx$ 590 ms).





% falar do laudo final com a panoramica



\subsection{Generated clinical report}
To visually corroborate the quantitative metrics and assess the boundary delineation accuracy, a qualitative analysis was performed. Figure \ref{fig:prompt_guided} presents a representative example of the segmentation results, displaying: 1. the raw panoramic radiograph, 2. the ground truth mask annotated by specialists, and 3. the final segmentation output generated by the integrated SAM1-FT and YOLOv12x architecture.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.85\textwidth]{img/plots/SAM1FT.jpg}
    \caption{Visual Comparison between the Model’s Prediction and the Ground Truth.}
    \label{fig:prompt_guided}
\end{figure*}

The application of the proposed framework yields the integrated results illustrated in Figure \ref{fig:json_report}. The visual output is presented in Figure \ref{fig:json_report}.(a), which displays the panoramic radiograph with the applied segmentation and classification masks. Correspondingly, Figure \ref{fig:json_report}.(b) shows the structured clinical report generated by ChatGPT-5, detailing the dental characteristics and findings identified within the image.

\begin{figure}[h!]
    \centering
    % Primeira figura (Topo)
    \subfloat[Visual output: Segmentation and classification.]{%
        \includegraphics[width=\linewidth]{img/plots/radiography_panoramic_report.jpg}
    }
    
    \vspace{0.5em} % Espaçamento vertical entre as figuras
    
    % Segunda figura (Abaixo)
    \subfloat[Textual output: Structured report (JSON).]{%
        \includegraphics[width=\linewidth]{img/plots/json_report.png}
        
    }

    \caption{Representative output of the proposed automated pipeline.}
    \label{fig:json_report}
\end{figure}





\subsection{Limitations of General-Purpose Largue Language Models in diagnostics}

The evaluation of ChatGPT-5 (Tables \ref{tab:gpt5_absent_present} and Figure \ref{fig:precision_recall_chatgpt5_laudo}) delineates the current boundary between specialized computer vision models and general-purpose Multimodal LLMs. While GPT-5 demonstrated a capability to identify macroscopic structures (present vs. absent teeth) with reasonable recall, it significantly underperformed in detecting subtle pathologies such as endodontic treatments, implants, and crown lesions.The superior performance of the YOLOv12m model (Precision 0.9764 vs. GPT-5 0.8585) reaffirms that, despite the advancements in generative AI, specialized discriminative models remain superior for tasks requiring high localization accuracy and domain-specific feature recognition. Furthermore, the inference time of GPT-5 ($\approx$ 3000 ms) is orders of magnitude higher than the dedicated CNNs ($<$ 100 ms). The prompt engineering experiments ("Guided" vs. "Zero-shot") revealed that while visual segmentation cues can reduce hallucination in counting tasks, they do not sufficiently enhance the model's ability to classify complex dental anomalies.


\section{Conclusion}

In conclusion, this study suggests a hybrid pipeline as the most effective solution for digital dentistry. A lightweight detector (YOLOv11n) provides rapid localization, which can serve as a prompt for a fine-tuned foundation model (SAM1-FT) to generate high-quality segmentation masks, eliminating the need for manual pixel-level annotation. While Large Language Models show promise for high-level reasoning, they currently lack the precision and efficiency required for primary diagnostic tasks in radiology. Future work should focus on the integration of these specialized vision outputs into LLM contexts to leverage the reasoning capabilities of the latter without relying on their visual processing limitations.
% ---- Informações extras
All code is availabe on github: \textit{ https://github.com/rodrigorochag/yoloSAM/tree/main} 












\bibliographystyle{elsarticle-num} 
\bibliography{doc/bibliography}

\appendix
\onecolumn % Necessário para longtable. Se for no meio do texto, quebrará a página. Ideal para Apêndices.

\begin{longtable}{p{\textwidth}}
\caption{Definition of the prompts used in the Zero-shot and Guided strategies.} 
\label{tab:appendix_all_prompts} \\
\toprule
\textbf{Zero-shot prompt} \\
\midrule
You are a specialist in dental radiology. Analyze the provided panoramic radiograph image and produce a structured report by filling in the following template in JSON format. Write according to World Dental Federation (FDI). \\
\addlinespace
\textbf{Important instructions:}
\begin{itemize}[nosep, leftmargin=*, label=\textbullet]
    \item Fill each field according to the findings visible in the image.
    \item Write each field only with numeric data.
    \item Write in ms the \texttt{"time"} field with the time spent on analysis.
    \item If nothing is detected, do not write.
    \item Do not add extra fields.
    \item Write the dentition type as "0" for permanent and "1" for mixed.
    \item The result must be in JSON format, mirroring the input's structure.
\end{itemize}
\addlinespace
\textbf{Formatting rules:}
\begin{itemize}[nosep, leftmargin=*, label=-]
    \item Provide only numeric FDI codes (e.g., 11, 16, 26, 36). No text.
    \item Use ascending order and avoid duplicates.
    \item If nothing is detected for a field, omit the field entirely.
\end{itemize} \\
\midrule
\textbf{Guided prompt} \\
\midrule
You are a specialist in dental radiology. Analyze the provided panoramic radiograph image and produce a structured report by filling in the following template in JSON format. Write according to World Dental Federation (FDI). \\
\addlinespace
\textbf{Important instructions:}
\begin{itemize}[nosep, leftmargin=*, label=\textbullet]
    \item Fill each field according to the findings visible in the image.
    \item Write each field only with numeric data.
    \item Write in ms the \texttt{"time"} field with the time spent on analysis.
    \item If nothing is detected, do not write.
    \item Do not add extra fields.
    \item Write the dentition type as "0" for permanent and "1" for mixed.
    \item The result must be in JSON format, mirroring the input's structure.
\end{itemize}
\addlinespace
\textbf{Field definitions (numeric-only output):}
\begin{itemize}[nosep, leftmargin=*, label=\textbullet]
    \item \textbf{Type of dentition} -- Output a single number: 0 (permanent) or 1 (mixed).
    \item \textbf{Endodontic treatment} -- List the FDI tooth numbers that show root canal treatment (radiopaque filling material/pin visible). Use ascending order and no duplicates.
    \item \textbf{Presence of implants} -- List the FDI positions that have dental implants visible on the radiograph. Use ascending order and no duplicates.
    \item \textbf{Tooth with mesial inclination} -- List the FDI teeth tilted toward the midline (mesial inclination). Ascending order, no duplicates.
    \item \textbf{Tooth with distal inclination} -- List the FDI teeth tilted away from the midline (distal inclination). Ascending order, no duplicates.
    \item \textbf{Crown lesions} -- List FDI teeth with coronal structural defects (e.g., fracture, loss of tooth structure, non-restorative defects) visible radiographically.
    \item \textbf{Impacted teeth (unerupted/retained)} -- List FDI teeth that have not erupted due to positional/mechanical obstruction (e.g., retained third molars).
    \item \textbf{Restorations} -- List FDI teeth with visible restorative materials (amalgam, composite, metallic/ceramic crowns, inlays/onlays).
    \item \textbf{Caries} -- List FDI teeth with radiographic evidence of carious lesions (radiolucencies at proximal/occlusal/cervical surfaces).
\end{itemize}
\addlinespace
\textbf{Formatting rules:}
\begin{itemize}[nosep, leftmargin=*, label=-]
    \item Provide only numeric FDI codes (e.g., 11, 16, 26, 36). No text.
    \item Use ascending order and avoid duplicates.
    \item If nothing is detected for a field, omit the field entirely.
\end{itemize} \\
\bottomrule
\end{longtable}

\twocolumn % Retorna para duas colunas após a tabela


\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
